{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training VanillaTCN on Shakespeare dataset for character-level prediction ",
   "id": "623cc213208796a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:19:22.865251Z",
     "start_time": "2025-11-08T14:19:22.524603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tcn import VanillaTCN"
   ],
   "id": "29f5f71ead790b89",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:29:08.249201Z",
     "start_time": "2025-11-08T14:29:08.232774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- hyperparameters ---\n",
    "# training\n",
    "learning_rate = 0.001\n",
    "rho_1, rho_2 = 0.9, 0.999  # Adam params\n",
    "dropout_input_p_keep = 1\n",
    "dropout_hidden_p_keep = 1\n",
    "\n",
    "batch_size = 5\n",
    "max_epochs = 100\n",
    "early_stop_rel_tol = 0\n",
    "data_size = 30  # limit data size for faster training, -1 for full data\n",
    "# number of backprop steps = O(batch_size * data_size * epochs)\n",
    "\n",
    "# model\n",
    "copies = 2  # copies of kernel-dilation list, so final_depth = depth * copies, see below\n",
    "depth = 3\n",
    "kernel_size = 3\n",
    "dilation_size = 2\n",
    "hidden_size = 20\n",
    "# num parameters = O(copies * depth * kernel_size * hidden_size^2)\n",
    "\n",
    "# generation\n",
    "seed_text = \"To be, or not to be\"\n",
    "generation_length = 50"
   ],
   "id": "c08a5f677a1a41d5",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:25:43.412589Z",
     "start_time": "2025-11-08T14:25:43.363639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- data ---\n",
    "file = \"input.txt\"\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "chars = ''.join(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "text = text[:data_size]"
   ],
   "id": "79a3a7b333a78ce0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:25:43.877937Z",
     "start_time": "2025-11-08T14:25:43.861660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- initialize model params --- \n",
    "dilations = copies * [dilation_size**i for i in range(depth-1, -1, -1)]\n",
    "kernel_sizes = copies * [kernel_size for _ in range(depth)]\n",
    "hidden_sizes = [vocab_size] + [hidden_size for _ in range(copies * depth-1)]"
   ],
   "id": "9b0d1145c4f19ba7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After including Adam the model stopped learning very well. I thought this might be due to poor weight initialization so here I do a quick hyperparameter search over initial weight scales to see how it affects gradient magnitudes. ",
   "id": "fc56a2746602db43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:25:45.554150Z",
     "start_time": "2025-11-08T14:25:44.721332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- weight initialization hyperparameter search ---\n",
    "minimum_weight_scale = 0.01\n",
    "maximum_weight_scale = 100\n",
    "runs = 20\n",
    "scale_grad = {}\n",
    "\n",
    "for run_idx in range(runs):\n",
    "    weight_scale = np.exp(np.random.uniform(np.log(minimum_weight_scale), np.log(maximum_weight_scale)))\n",
    "    \n",
    "    # generate some data\n",
    "    model = VanillaTCN(input_size=vocab_size, dilations=dilations,\n",
    "                   kernel_sizes=kernel_sizes, hidden_sizes=hidden_sizes,\n",
    "                   input_p_keep=dropout_input_p_keep, hidden_p_keep=dropout_hidden_p_keep)\n",
    "    T_f = model.T_f\n",
    "    inputs_batch = np.zeros((batch_size, vocab_size, T_f), dtype=np.float32)\n",
    "    targets_batch = np.zeros((batch_size, vocab_size), dtype=np.float32)\n",
    "    i = np.random.randint(T_f, len(text) - batch_size - 1)\n",
    "    for b in range(batch_size):\n",
    "        for t in range(T_f):\n",
    "            inputs_batch[b, char_to_idx[text[i - T_f + t + b]], t] = 1\n",
    "        targets_batch[b, char_to_idx[text[i + b]]] = 1\n",
    "    \n",
    "    # get gradients\n",
    "    model.train_minibatch(inputs_batch, targets_batch,\n",
    "                          rho_1=rho_1, rho_2=rho_2,\n",
    "                          learning_rate=learning_rate)\n",
    "    grads = model.dw\n",
    "    scale_grad[weight_scale] = [(np.mean(np.abs(g)), np.std(np.abs(g))) for g in grads]\n",
    "    print(weight_scale, [(np.mean(np.abs(g)), np.std(np.abs(g))) for g in grads])"
   ],
   "id": "1d5d805428cf6c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026104827896618176 [(0.003359679, 0.008232633), (0.0044815876, 0.006521489), (0.0032658903, 0.0045701764), (0.003278421, 0.0033071951), (0.0034511718, 0.0028297934), (0.00061457267, 0.0016448857)]\n",
      "0.05276683829540094 [(0.0038342564, 0.0078014946), (0.008722995, 0.010834441), (0.005951725, 0.007367826), (0.005474142, 0.005208902), (0.003949897, 0.003381496), (0.00067000854, 0.0017621431)]\n",
      "1.6983074659463868 [(0.002761631, 0.008531206), (0.007836726, 0.010102911), (0.0043202764, 0.004953173), (0.0032497128, 0.0033437093), (0.0025253652, 0.0023581134), (0.0004771137, 0.0013385761)]\n",
      "0.054060586007368336 [(0.003611302, 0.008964293), (0.006593332, 0.008332121), (0.005032525, 0.0062540867), (0.004380226, 0.0053018574), (0.0027632103, 0.0027429617), (0.00048393215, 0.0013281257)]\n",
      "0.10393266587460642 [(0.0031713026, 0.008267327), (0.0068579405, 0.009661908), (0.005112388, 0.006214905), (0.003232685, 0.0037204595), (0.0023775585, 0.002088501), (0.00056070293, 0.0016528649)]\n",
      "71.03761078350907 [(0.0020018877, 0.005608063), (0.007490638, 0.010633458), (0.005210728, 0.006417679), (0.0027088749, 0.0026826076), (0.002531959, 0.0021467714), (0.0005215684, 0.0014131184)]\n",
      "0.135624794439847 [(0.003079812, 0.007037131), (0.0056411973, 0.00840845), (0.0036627133, 0.0041262764), (0.002740099, 0.0029819745), (0.002719721, 0.0023631833), (0.0006318322, 0.0017701796)]\n",
      "0.025503754596205734 [(0.002398063, 0.0050771423), (0.004535376, 0.0056350166), (0.0036813014, 0.004636007), (0.00269878, 0.0027123317), (0.0023519048, 0.0020608604), (0.0005280559, 0.0014985238)]\n",
      "1.1671582457665275 [(0.0030437093, 0.00563927), (0.0042539695, 0.005731063), (0.004007191, 0.0043059117), (0.0034736474, 0.0036326693), (0.0030268151, 0.0024866473), (0.0005488917, 0.0014804035)]\n",
      "70.13851294139182 [(0.0030362976, 0.008256754), (0.0077025173, 0.009485079), (0.004385585, 0.0041565173), (0.0030671582, 0.003038597), (0.0023971288, 0.0020498454), (0.00048287914, 0.0013544103)]\n",
      "47.72269746907709 [(0.0033923928, 0.007803758), (0.004913404, 0.0065776147), (0.0045543145, 0.0052076755), (0.0045302035, 0.0048112655), (0.003207995, 0.002898347), (0.00065878127, 0.001852225)]\n",
      "16.876196887058452 [(0.0020662867, 0.004772895), (0.004892185, 0.0064296992), (0.00391394, 0.004184312), (0.004021155, 0.004505841), (0.0038997063, 0.0035302348), (0.0006105513, 0.0017974977)]\n",
      "15.3970522676198 [(0.002882666, 0.006740804), (0.0075796125, 0.008799495), (0.0053868457, 0.0071472498), (0.0050838557, 0.006401668), (0.0045379973, 0.0036060493), (0.00068990013, 0.0018876577)]\n",
      "0.07631348423718444 [(0.0032864893, 0.007755626), (0.0059323404, 0.0071758972), (0.004848007, 0.0060387426), (0.004721321, 0.0040462445), (0.0030653854, 0.00270976), (0.00069480075, 0.0018991152)]\n",
      "0.035385125407633986 [(0.003083701, 0.008015975), (0.0052309325, 0.0066242726), (0.004466714, 0.005577191), (0.003218504, 0.003135154), (0.0028498531, 0.0023151194), (0.00059404335, 0.0016084238)]\n",
      "8.828339049053024 [(0.0024300136, 0.0055135963), (0.0045400998, 0.0073078866), (0.0051530204, 0.0067117014), (0.004175213, 0.004767761), (0.0029619858, 0.0025171717), (0.00067616696, 0.0018958914)]\n",
      "1.798545296498737 [(0.002833163, 0.007323191), (0.0074790325, 0.00973207), (0.006251223, 0.0074687605), (0.0046249423, 0.0046740845), (0.003486174, 0.003009758), (0.0006431025, 0.0017908368)]\n",
      "32.16277130667638 [(0.003635362, 0.008691327), (0.0069002365, 0.010124959), (0.004843622, 0.0046673706), (0.0030785291, 0.0032883098), (0.0026224605, 0.002366137), (0.00050826056, 0.0014354422)]\n",
      "1.3295977582483252 [(0.0025733097, 0.0064273477), (0.006493231, 0.007874335), (0.005758242, 0.0067691724), (0.0037610673, 0.004065294), (0.0037801806, 0.0032673792), (0.0007258418, 0.0019915346)]\n",
      "40.94512285619302 [(0.0030464847, 0.006389487), (0.004829243, 0.006729883), (0.0036185305, 0.004064779), (0.0028945212, 0.0026062992), (0.0032364817, 0.0029139207), (0.00072204944, 0.0021484932)]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see that the gradients (in particular note the standard deviation) are actually surprisingly insensitive to initial weight scale. So we just fix it to 0.1 for the rest of the training.",
   "id": "6978219e4f460586"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:25:45.584313Z",
     "start_time": "2025-11-08T14:25:45.559488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model init\n",
    "dilations = copies * [dilation_size**i for i in range(depth-1, -1, -1)]\n",
    "kernel_sizes = copies * [kernel_size for _ in range(depth)]\n",
    "hidden_sizes = [vocab_size] + [hidden_size for _ in range(copies * depth-1)]\n",
    "model = VanillaTCN(input_size=vocab_size, dilations=dilations,\n",
    "                   kernel_sizes=kernel_sizes, hidden_sizes=hidden_sizes,\n",
    "                   input_p_keep=dropout_input_p_keep, hidden_p_keep=dropout_hidden_p_keep,\n",
    "                   weight_scale=0.1)\n",
    "T_f = model.T_f\n",
    "\n",
    "# --- print info ---\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(f\"proportion ensemble disconnected approx: {(1-dropout_hidden_p_keep)**sum(model.used_node_idx[2]):.0e}\")\n",
    "print(\"Receptive field:\", T_f)\n",
    "print(\"Parameters:\", sum([w.size for w in model.weights]) + sum([b.size for b in model.biases]))\n",
    "print(f\"Expected number of total backprop evals: {batch_size * (len(text)-T_f) * max_epochs:.2e}\")\n",
    "# (depth-2)*(kernel_size*hidden_size**2+hidden_size)+2*kernel_size*hidden_size*vocab_size+vocab_size+hidden_size"
   ],
   "id": "12d251885bb74198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "proportion ensemble disconnected approx: 0e+00\n",
      "Receptive field: 29\n",
      "Parameters: 12765\n",
      "Expected number of total backprop evals: 5.50e+03\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:31:02.246806Z",
     "start_time": "2025-11-08T14:30:58.961991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- training loop ---\n",
    "t0 = time.time()\n",
    "prev_loss = 1e6\n",
    "end = len(text) - batch_size - 1\n",
    "for e in range(max_epochs):\n",
    "    for i in range(T_f, end+1):\n",
    "        inputs_batch = np.zeros((batch_size, vocab_size, T_f), dtype=np.float32)\n",
    "        targets_batch = np.zeros((batch_size, vocab_size), dtype=np.float32)\n",
    "        for b in range(batch_size):\n",
    "            for t in range(T_f):\n",
    "                inputs_batch[b, char_to_idx[text[i - T_f + t + b]], t] = 1\n",
    "            targets_batch[b, char_to_idx[text[i + b]]] = 1\n",
    "        model.train_minibatch(inputs_batch, targets_batch,\n",
    "                                  rho_1=rho_1, rho_2=rho_2,\n",
    "                                  learning_rate=learning_rate)\n",
    "    \n",
    "    loss = model.train_minibatch(inputs_batch, targets_batch,\n",
    "                                  return_loss=True)\n",
    "    print(f\"Epoch {e+1}/{max_epochs}, Loss (on final batch): {loss:.4f} \"\n",
    "          f\"(i.e. avg. prob. assigned: {np.exp(-loss):.4f}) \"\n",
    "          f\"time elapsed: {time.time() - t0:.0f}s eta: {(time.time() - t0)/(e+1)*(max_epochs - e - 1)/60:.1f}m\")\n",
    "\n",
    "    # --- generate text ---\n",
    "    if len(seed_text) < T_f:\n",
    "        test = \" \" * (T_f - len(seed_text)) + seed_text\n",
    "    else:\n",
    "        test = seed_text[-T_f:]\n",
    "        print(\"Warning: seed_text length greater than receptive field size.\")\n",
    "    for i in range(generation_length):\n",
    "        input_seq = np.zeros((vocab_size, T_f), dtype=np.float32)\n",
    "        for t in range(T_f):\n",
    "            input_seq[char_to_idx[test[-T_f + t]], t] = 1\n",
    "        output_probs = model.forward_pass(input_seq, do_dropout=False)\n",
    "        next_char_idx = np.random.choice(range(vocab_size), p=output_probs.ravel())\n",
    "        test += idx_to_char[next_char_idx]\n",
    "\n",
    "    print(\"Generated text:\")\n",
    "    print(test, '\\n')\n",
    "\n",
    "    # Stopped learning?\n",
    "    if abs(prev_loss - loss)/max(loss, prev_loss, 1e-8) < early_stop_rel_tol:\n",
    "        print(\"Converged, stopping training\")\n",
    "        break\n",
    "    prev_loss = loss"
   ],
   "id": "d2dd69098b535b3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.03242042288184166\n",
      "Epoch 1/100, Loss (on final batch): 2.8847 (i.e. avg. prob. assigned: 0.0559) time elapsed: 0s eta: 0.4m\n",
      "Generated text:\n",
      "          To be, or not to be KC n dnd yyyn-nnnae yye ndyay an daeenedaaydyna a \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.03244110941886902\n",
      "Epoch 2/100, Loss (on final batch): 2.8831 (i.e. avg. prob. assigned: 0.0560) time elapsed: 1s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to bennanaOf danK-dc  nendW y  se raaeQylyn'yeendfn yan \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.03248860687017441\n",
      "Epoch 3/100, Loss (on final batch): 2.8816 (i.e. avg. prob. assigned: 0.0560) time elapsed: 1s eta: 0.6m\n",
      "Generated text:\n",
      "          To be, or not to benaynddd ayy:dgdy ay nyyn y fenddfynadyadnn e e Kny \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.032537173479795456\n",
      "Epoch 4/100, Loss (on final batch): 2.8800 (i.e. avg. prob. assigned: 0.0561) time elapsed: 1s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to beyy adnnr  ayadanyd  nn ak dKn  lydnen G wCnn n dfd \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.03254536911845207\n",
      "Epoch 5/100, Loss (on final batch): 2.8784 (i.e. avg. prob. assigned: 0.0562) time elapsed: 2s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to beeen;eyNyS   nf dde'  ndysnsaUa n nayeyyy aa  aVaay \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.032028678804636\n",
      "Epoch 6/100, Loss (on final batch): 2.8768 (i.e. avg. prob. assigned: 0.0563) time elapsed: 2s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to beaeyrea n afanHa redyaenda  nanyfe ya ayn- ynydf ny \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.0320792980492115\n",
      "Epoch 7/100, Loss (on final batch): 2.8752 (i.e. avg. prob. assigned: 0.0564) time elapsed: 2s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to beyyyna ydna fy nf aadnda ydfdeanjndnaf  dnneae yaaa \n",
      "\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Average gradient norm: 0.03265184536576271\n",
      "Epoch 8/100, Loss (on final batch): 2.8736 (i.e. avg. prob. assigned: 0.0565) time elapsed: 3s eta: 0.5m\n",
      "Generated text:\n",
      "          To be, or not to beaUddnC  y yne npenennyXanan n  C afdnaaaad a xaLyy \n",
      "\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m             inputs_batch[b, char_to_idx[text[i \u001B[38;5;241m-\u001B[39m T_f \u001B[38;5;241m+\u001B[39m t \u001B[38;5;241m+\u001B[39m b]], t] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     13\u001B[0m         targets_batch[b, char_to_idx[text[i \u001B[38;5;241m+\u001B[39m b]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 14\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_minibatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mrho_1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrho_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrho_2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrho_2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtrain_minibatch(inputs_batch, targets_batch,\n\u001B[0;32m     19\u001B[0m                               return_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss (on final batch): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     21\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(i.e. avg. prob. assigned: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mloss)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     22\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime elapsed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mt0\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.0f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms eta: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(time\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mt0)\u001B[38;5;241m/\u001B[39m(e\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39m(max_epochs\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39me\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\Python\\quant-gan-numerics-program\\TCN Shakespeare\\tcn.py:76\u001B[0m, in \u001B[0;36mVanillaTCN.train_minibatch\u001B[1;34m(self, inputs_batch, targets_batch, learning_rate, rho_1, rho_2, return_loss)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_grads(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(m):\n\u001B[1;32m---> 76\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_dropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mreturn_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_loss:\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAverage gradient norm: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean([np\u001B[38;5;241m.\u001B[39mabs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdw[i])\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mi\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepth)])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\Python\\quant-gan-numerics-program\\TCN Shakespeare\\tcn.py:98\u001B[0m, in \u001B[0;36mVanillaTCN.update_gradients\u001B[1;34m(self, inputs, target, do_dropout)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, target, do_dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     97\u001B[0m     output, loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_pass(inputs, target, do_dropout\u001B[38;5;241m=\u001B[39mdo_dropout)\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mback_prop\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\Desktop\\Python\\quant-gan-numerics-program\\TCN Shakespeare\\tcn.py:127\u001B[0m, in \u001B[0;36mVanillaTCN.back_prop\u001B[1;34m(self, output, target)\u001B[0m\n\u001B[0;32m    124\u001B[0m     flat_idx \u001B[38;5;241m=\u001B[39m idx_prev\u001B[38;5;241m.\u001B[39mravel()\n\u001B[0;32m    125\u001B[0m     np\u001B[38;5;241m.\u001B[39madd\u001B[38;5;241m.\u001B[39mat(dL_df, (\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m), flat_idx), contrib_flat)\n\u001B[1;32m--> 127\u001B[0m dL_dphi \u001B[38;5;241m=\u001B[39m dL_df \u001B[38;5;241m*\u001B[39m \u001B[43mgrad_leaky_re_lu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_vals\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m js \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mused_node_idx[i]\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m js\u001B[38;5;241m.\u001B[39msize:\n",
      "File \u001B[1;32m~\\Desktop\\Python\\quant-gan-numerics-program\\TCN Shakespeare\\tcn.py:8\u001B[0m, in \u001B[0;36mgrad_leaky_re_lu\u001B[1;34m(x)\u001B[0m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrad_leaky_re_lu\u001B[39m(x): \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "603cb08fa4f0fea3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
